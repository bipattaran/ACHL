{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9YLEVheOIkF"
      },
      "outputs": [],
      "source": [
        "# =======================\n",
        "#  CELL 1\n",
        "#  Step 1: Data Fetching & Setup\n",
        "# =======================\n",
        "\n",
        "import sys\n",
        "\n",
        "import datetime\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "try:\n",
        "    import yfinance as yf\n",
        "except ImportError:\n",
        "    print(\"Please install yfinance (pip install yfinance) before proceeding.\")\n",
        "    # In Colab, you can do: !pip install yfinance\n",
        "    raise\n",
        "\n",
        "def fetch_data_from_yahoo(tickers, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Fetch Adjusted Close data from Yahoo Finance for given tickers and date range.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    - tickers: list of str\n",
        "    - start_date: datetime or str\n",
        "    - end_date: datetime or str\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    - pandas.DataFrame: Adjusted Close prices, rows are dates, columns are tickers\n",
        "    \"\"\"\n",
        "    data = yf.download(tickers, start=start_date, end=end_date)['Close']\n",
        "    # Forward-fill any missing data, then drop remaining NAs\n",
        "    data.ffill(inplace=True)\n",
        "    data.dropna(inplace=True)\n",
        "    return data\n",
        "\n",
        "# Example Tickers for top 20 NSE\n",
        "nse_tickers = [\n",
        "    'RELIANCE.NS', 'TCS.NS', 'HDFCBANK.NS', 'INFY.NS', 'ICICIBANK.NS',\n",
        "    'HINDUNILVR.NS', 'KOTAKBANK.NS', 'LT.NS', 'SBIN.NS', 'AXISBANK.NS',\n",
        "    'HCLTECH.NS', 'BAJFINANCE.NS', 'ITC.NS', 'BHARTIARTL.NS', 'ASIANPAINT.NS',\n",
        "    'MARUTI.NS', 'SUNPHARMA.NS', 'TITAN.NS', 'WIPRO.NS', 'ULTRACEMCO.NS'\n",
        "]\n",
        "\n",
        "# Fetch Data\n",
        "end_date = datetime.datetime.today()\n",
        "start_date = end_date - datetime.timedelta(days=720)  # ~2 years\n",
        "\n",
        "print(\"Fetching sample data from Yahoo Finance in Colab environment...\")\n",
        "try:\n",
        "    price_data = fetch_data_from_yahoo(nse_tickers, start_date, end_date)\n",
        "    print(f\"Data fetched successfully! Shape: {price_data.shape}\")\n",
        "    print(f\"Date Range: {price_data.index.min()} to {price_data.index.max()}\")\n",
        "    print(f\"First few rows:\\n{price_data.head()}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error fetching data: {e}\")\n",
        "    price_data = None\n",
        "\n",
        "print(\"STEP 1 completed in Colab cell!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "#  CELL 2 - Revised\n",
        "#  Step 2: Classic ML Models (Per-Asset Neural Network and XGBoost)\n",
        "# =======================\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from scikeras.wrappers import KerasRegressor  # Correct import for scikeras\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import xgboost as xgb\n",
        "from math import sqrt\n",
        "import joblib  # For saving/loading models\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "###############################################\n",
        "# 1. Extended Feature Engineering\n",
        "###############################################\n",
        "\n",
        "def compute_daily_returns(prices_df):\n",
        "    \"\"\"Compute daily returns from price data.\"\"\"\n",
        "    return prices_df.pct_change().dropna()\n",
        "\n",
        "def compute_rolling_volatility(returns_df, window=20):\n",
        "    \"\"\"Compute rolling volatility over a given window.\"\"\"\n",
        "    return returns_df.rolling(window).std().dropna()\n",
        "\n",
        "def compute_ma(prices_df, window=20):\n",
        "    \"\"\"Compute moving average.\"\"\"\n",
        "    return prices_df.rolling(window).mean().dropna()\n",
        "\n",
        "def compute_rsi(prices_df, window=14):\n",
        "    \"\"\"Compute RSI for each column in prices_df.\"\"\"\n",
        "    def _single_rsi(series):\n",
        "        delta = series.diff().dropna()\n",
        "        gain = delta.where(delta > 0, 0).rolling(window=window).mean()\n",
        "        loss = -delta.where(delta < 0, 0).rolling(window=window).mean()\n",
        "        rs = gain / (loss + 1e-15)\n",
        "        return 100 - (100 / (1 + rs))\n",
        "    return prices_df.apply(_single_rsi).dropna()\n",
        "\n",
        "def build_feature_matrix(prices_df):\n",
        "    \"\"\"\n",
        "    Combine daily returns, rolling volatility, 20-day MA, and RSI(14).\n",
        "    Shift next-day returns as the target.\n",
        "    \"\"\"\n",
        "    # 1) Daily returns\n",
        "    daily_ret = compute_daily_returns(prices_df)\n",
        "\n",
        "    # 2) Rolling volatility on daily returns\n",
        "    vol = compute_rolling_volatility(daily_ret, 20)\n",
        "\n",
        "    # 3) 20-day Moving Average\n",
        "    ma20 = compute_ma(prices_df, 20)\n",
        "\n",
        "    # 4) RSI(14)\n",
        "    rsi = compute_rsi(prices_df, 14)\n",
        "\n",
        "    # Combine all features\n",
        "    combined = pd.concat([\n",
        "        daily_ret.add_suffix('_Ret'),\n",
        "        vol.add_suffix('_Vol'),\n",
        "        ma20.add_suffix('_MA20'),\n",
        "        rsi.add_suffix('_RSI')\n",
        "    ], axis=1).dropna()\n",
        "\n",
        "    # Shift next-day returns as target\n",
        "    shifted = daily_ret.shift(-1).dropna()\n",
        "\n",
        "    # Align the feature matrix with the shifted targets\n",
        "    combined = combined.iloc[:-1]  # Drop the last row to match the shifted target\n",
        "    combined.dropna(inplace=True)\n",
        "    shifted.dropna(inplace=True)\n",
        "\n",
        "    # Final alignment\n",
        "    X, y = combined.align(shifted, join='inner', axis=0)\n",
        "    return X, y\n",
        "\n",
        "###############################################\n",
        "# 2. Train Per-Asset Neural Network Models\n",
        "###############################################\n",
        "\n",
        "def create_keras_model(input_dim=24, output_dim=1, n_hidden=1, n_neurons=32, dropout=0.2, lr=0.001):\n",
        "    \"\"\"Build a Keras model based on given hyperparameters.\"\"\"\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Input(shape=(input_dim,)))\n",
        "    for _ in range(n_hidden):\n",
        "        model.add(layers.Dense(n_neurons, activation='relu'))\n",
        "        model.add(layers.Dropout(dropout))\n",
        "    model.add(layers.Dense(output_dim, activation='linear'))\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "    return model\n",
        "\n",
        "def tune_neural_network(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Tune a Neural Network using scikeras's KerasRegressor and RandomizedSearchCV.\n",
        "    \"\"\"\n",
        "    input_dim = X_train.shape[1]\n",
        "    output_dim = 1  # Per asset\n",
        "\n",
        "    # Create the KerasRegressor with a fixed input/output shape\n",
        "    keras_reg = KerasRegressor(\n",
        "        model=create_keras_model,\n",
        "        input_dim=input_dim,  # Fixed input dimension\n",
        "        output_dim=output_dim,  # Fixed output dimension\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Define the hyperparameter grid\n",
        "    param_distributions = {\n",
        "        'model__n_hidden': [1, 2],\n",
        "        'model__n_neurons': [32, 64],\n",
        "        'model__dropout': [0.2, 0.3],\n",
        "        'model__lr': [1e-3, 1e-4],\n",
        "        'epochs': [30, 50],\n",
        "        'batch_size': [32, 64]\n",
        "    }\n",
        "\n",
        "    # Define TimeSeriesSplit for cross-validation\n",
        "    tscv = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "    # Initialize RandomizedSearchCV\n",
        "    search = RandomizedSearchCV(\n",
        "        estimator=keras_reg,\n",
        "        param_distributions=param_distributions,\n",
        "        n_iter=6,  # Increase for more thorough search\n",
        "        cv=tscv,\n",
        "        scoring='neg_mean_squared_error',\n",
        "        random_state=SEED,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Fit the model\n",
        "    search.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"Best NN params= {search.best_params_}\")\n",
        "    best_model = search.best_estimator_\n",
        "    return best_model\n",
        "\n",
        "def train_neural_network_per_asset(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Train Neural Network models for each asset's DeltaR and DeltaSigma.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    - dict: {'DeltaR': {asset: model, ...}, 'DeltaSigma': {asset: model, ...}}\n",
        "    \"\"\"\n",
        "    model_dict = {'DeltaR': {}, 'DeltaSigma': {}}\n",
        "    assets = y_train.columns\n",
        "\n",
        "    for asset in assets:\n",
        "        print(f\"Training Neural Network for DeltaR of asset: {asset}\")\n",
        "        # Train model for DeltaR\n",
        "        nn_model_r = tune_neural_network(X_train, y_train[asset])\n",
        "        model_dict['DeltaR'][asset] = nn_model_r\n",
        "\n",
        "        print(f\"Training Neural Network for DeltaSigma of asset: {asset}\")\n",
        "        # Train model for DeltaSigma\n",
        "        nn_model_s = tune_neural_network(X_train, y_train[asset])\n",
        "        model_dict['DeltaSigma'][asset] = nn_model_s\n",
        "\n",
        "    return model_dict\n",
        "\n",
        "def evaluate_neural_networks_per_asset(model_dict, X, y):\n",
        "    \"\"\"\n",
        "    Evaluate the Neural Network models for each asset's DeltaR and DeltaSigma.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    - dict: {'DeltaR': {asset: rmse, ...}, 'DeltaSigma': {asset: rmse, ...}}\n",
        "    \"\"\"\n",
        "    rmse_dict = {'DeltaR': {}, 'DeltaSigma': {}}\n",
        "    assets = y.columns\n",
        "\n",
        "    for asset in assets:\n",
        "        # Evaluate DeltaR\n",
        "        pred_r = model_dict['DeltaR'][asset].predict(X)\n",
        "        rmse_r = sqrt(mean_squared_error(y[asset], pred_r))\n",
        "        rmse_dict['DeltaR'][asset] = rmse_r\n",
        "\n",
        "        # Evaluate DeltaSigma\n",
        "        pred_s = model_dict['DeltaSigma'][asset].predict(X)\n",
        "        rmse_s = sqrt(mean_squared_error(y[asset], pred_s))\n",
        "        rmse_dict['DeltaSigma'][asset] = rmse_s\n",
        "\n",
        "        print(f\"[NeuralNet/{asset}/DeltaR] RMSE= {rmse_r:.5f}\")\n",
        "        print(f\"[NeuralNet/{asset}/DeltaSigma] RMSE= {rmse_s:.5f}\")\n",
        "\n",
        "    return rmse_dict\n",
        "\n",
        "def example_usage_neural_network_per_asset(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Full demonstration:\n",
        "      1) Train Neural Network models for each asset's DeltaR and DeltaSigma.\n",
        "      2) Evaluate on training and testing data.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    - dict: {'DeltaR': {asset: model, ...}, 'DeltaSigma': {asset: model, ...}}\n",
        "    \"\"\"\n",
        "    # 1) Train models\n",
        "    model_dict = train_neural_network_per_asset(X_train, X_test, y_train, y_test)\n",
        "\n",
        "    # 2) Evaluate on training data\n",
        "    print(\"Evaluating Neural Network models on training data...\")\n",
        "    train_rmse = evaluate_neural_networks_per_asset(model_dict, X_train, y_train)\n",
        "\n",
        "    # 3) Evaluate on testing data\n",
        "    print(\"Evaluating Neural Network models on testing data...\")\n",
        "    test_rmse = evaluate_neural_networks_per_asset(model_dict, X_test, y_test)\n",
        "\n",
        "    # Summary\n",
        "    print(\"=== Summary of Neural Network Models ===\")\n",
        "    for asset in y_train.columns:\n",
        "        print(f\"Asset: {asset}\")\n",
        "        print(f\"  DeltaR Test RMSE: {test_rmse['DeltaR'][asset]:.5f}\")\n",
        "        print(f\"  DeltaSigma Test RMSE: {test_rmse['DeltaSigma'][asset]:.5f}\")\n",
        "\n",
        "    return model_dict\n",
        "\n",
        "###############################################\n",
        "# 3. Train Per-Asset XGBoost Models\n",
        "###############################################\n",
        "\n",
        "def train_xgboost_models_per_asset(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Train separate XGBoost models for each asset's DeltaR and DeltaSigma.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    - dict: {'DeltaR': {asset: model, ...}, 'DeltaSigma': {asset: model, ...}}\n",
        "    \"\"\"\n",
        "    model_dict = {'DeltaR': {}, 'DeltaSigma': {}}\n",
        "    assets = y_train.columns\n",
        "    tscv = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "    for asset in assets:\n",
        "        print(f\"Training XGBoost model for DeltaR of asset: {asset}\")\n",
        "        # Train model for DeltaR\n",
        "        xgb_model_r = xgb.XGBRegressor(objective='reg:squarederror', random_state=SEED)\n",
        "        param_grid_r = {\n",
        "            'n_estimators': [100, 200],\n",
        "            'max_depth': [3, 5],\n",
        "            'learning_rate': [0.01, 0.1]\n",
        "        }\n",
        "        grid_r = GridSearchCV(\n",
        "            estimator=xgb_model_r,\n",
        "            param_grid=param_grid_r,\n",
        "            cv=tscv,\n",
        "            scoring='neg_mean_squared_error',\n",
        "            verbose=0\n",
        "        )\n",
        "        grid_r.fit(X_train, y_train[asset])\n",
        "        print(f\"Best Params for DeltaR ({asset}): {grid_r.best_params_}\")\n",
        "        model_dict['DeltaR'][asset] = grid_r.best_estimator_\n",
        "\n",
        "        print(f\"Training XGBoost model for DeltaSigma of asset: {asset}\")\n",
        "        # Train model for DeltaSigma\n",
        "        xgb_model_s = xgb.XGBRegressor(objective='reg:squarederror', random_state=SEED)\n",
        "        param_grid_s = {\n",
        "            'n_estimators': [100, 200],\n",
        "            'max_depth': [3, 5],\n",
        "            'learning_rate': [0.01, 0.1]\n",
        "        }\n",
        "        grid_s = GridSearchCV(\n",
        "            estimator=xgb_model_s,\n",
        "            param_grid=param_grid_s,\n",
        "            cv=tscv,\n",
        "            scoring='neg_mean_squared_error',\n",
        "            verbose=0\n",
        "        )\n",
        "        grid_s.fit(X_train, y_train[asset])\n",
        "        print(f\"Best Params for DeltaSigma ({asset}): {grid_s.best_params_}\")\n",
        "        model_dict['DeltaSigma'][asset] = grid_s.best_estimator_\n",
        "\n",
        "    return model_dict\n",
        "\n",
        "def evaluate_xgboost_models_per_asset(model_dict, X, y):\n",
        "    \"\"\"\n",
        "    Evaluate the XGBoost models for each asset's DeltaR and DeltaSigma.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    - dict: {'DeltaR': {asset: rmse, ...}, 'DeltaSigma': {asset: rmse, ...}}\n",
        "    \"\"\"\n",
        "    rmse_dict = {'DeltaR': {}, 'DeltaSigma': {}}\n",
        "    assets = y.columns\n",
        "\n",
        "    for asset in assets:\n",
        "        # Evaluate DeltaR\n",
        "        pred_r = model_dict['DeltaR'][asset].predict(X)\n",
        "        rmse_r = sqrt(mean_squared_error(y[asset], pred_r))\n",
        "        rmse_dict['DeltaR'][asset] = rmse_r\n",
        "\n",
        "        # Evaluate DeltaSigma\n",
        "        pred_s = model_dict['DeltaSigma'][asset].predict(X)\n",
        "        rmse_s = sqrt(mean_squared_error(y[asset], pred_s))\n",
        "        rmse_dict['DeltaSigma'][asset] = rmse_s\n",
        "\n",
        "        print(f\"[XGBoost/{asset}/DeltaR] RMSE= {rmse_r:.5f}\")\n",
        "        print(f\"[XGBoost/{asset}/DeltaSigma] RMSE= {rmse_s:.5f}\")\n",
        "\n",
        "    return rmse_dict\n",
        "\n",
        "def example_usage_xgboost_per_asset(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Full demonstration:\n",
        "      1) Train XGBoost models for each asset's DeltaR and DeltaSigma.\n",
        "      2) Evaluate on training and testing data.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    - dict: {'DeltaR': {asset: model, ...}, 'DeltaSigma': {asset: model, ...}}\n",
        "    \"\"\"\n",
        "    # 1) Train models\n",
        "    model_dict = train_xgboost_models_per_asset(X_train, y_train)\n",
        "\n",
        "    # 2) Evaluate on training data\n",
        "    print(\"Evaluating XGBoost models on training data...\")\n",
        "    train_rmse = evaluate_xgboost_models_per_asset(model_dict, X_train, y_train)\n",
        "\n",
        "    # 3) Evaluate on testing data\n",
        "    print(\"Evaluating XGBoost models on testing data...\")\n",
        "    test_rmse = evaluate_xgboost_models_per_asset(model_dict, X_test, y_test)\n",
        "\n",
        "    # Summary\n",
        "    print(\"=== Summary of XGBoost Models ===\")\n",
        "    for asset in y_train.columns:\n",
        "        print(f\"Asset: {asset}\")\n",
        "        print(f\"  DeltaR Test RMSE: {test_rmse['DeltaR'][asset]:.5f}\")\n",
        "        print(f\"  DeltaSigma Test RMSE: {test_rmse['DeltaSigma'][asset]:.5f}\")\n",
        "\n",
        "    return model_dict\n",
        "\n",
        "###############################################\n",
        "# 4. Main Step 2 Logic with Extended Features and Hyperparameter Tuning\n",
        "###############################################\n",
        "\n",
        "if 'price_data' not in globals() or price_data is None:\n",
        "    print(\"price_data not found. Please run the Step 1 cell first!\")\n",
        "else:\n",
        "    print(\"Building extended feature matrix & shifted target (next-day returns)...\")\n",
        "    X, y = build_feature_matrix(price_data)\n",
        "    print(f\"Feature matrix shape= {X.shape}, target shape= {y.shape}\")\n",
        "\n",
        "    # Train/Test Split\n",
        "    def time_series_split_data(X, y, test_size=0.2):\n",
        "        \"\"\"\n",
        "        Split data into train and test sets without shuffling.\n",
        "        \"\"\"\n",
        "        n = len(X)\n",
        "        split_idx = int(n * (1 - test_size))\n",
        "        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
        "        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    X_train, X_test, y_train, y_test = time_series_split_data(X, y, test_size=0.2)\n",
        "    print(f\"X_train= {X_train.shape}, X_test= {X_test.shape}\")\n",
        "\n",
        "    # Train Neural Network models per asset\n",
        "    print(\"Training Neural Network models for each asset's DeltaR and DeltaSigma...\")\n",
        "    nn_models = example_usage_neural_network_per_asset(X_train, X_test, y_train, y_test)\n",
        "\n",
        "    # Train XGBoost models per asset\n",
        "    print(\"Training XGBoost models for each asset's DeltaR and DeltaSigma...\")\n",
        "    xgb_models = example_usage_xgboost_per_asset(X_train, X_test, y_train, y_test)\n",
        "\n",
        "    print(\"STEP 2 completed (extended features, hyperparameter tuning for Neural Networks and XGBoost)!\")\n"
      ],
      "metadata": {
        "id": "6zYzPnZTbhUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "#  CELL 3 - Final Revised with Ensemble and Enhanced CV\n",
        "#  Step 3: GNN Integration (Post-Installation)\n",
        "# =======================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import dcor  # For distance correlation\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler  # For target normalization\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "###############################################\n",
        "# 1. Define GNN Models with Different Aggregators\n",
        "###############################################\n",
        "\n",
        "class MarginalGNN(nn.Module):\n",
        "    \"\"\"\n",
        "    A 3-layer Graph Convolutional Network for predicting [DeltaR, DeltaSigma] per asset.\n",
        "    Supports different aggregator types: 'gcn', 'sage', 'gat'.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, hidden_channels: int = 64, out_channels: int = 2, gnn_type: str = 'gcn'):\n",
        "        super(MarginalGNN, self).__init__()\n",
        "\n",
        "        if gnn_type == 'gcn':\n",
        "            self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "            self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "            self.conv3 = GCNConv(hidden_channels, out_channels)\n",
        "        elif gnn_type == 'sage':\n",
        "            self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "            self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
        "            self.conv3 = SAGEConv(hidden_channels, out_channels)\n",
        "        elif gnn_type == 'gat':\n",
        "            self.conv1 = GATConv(in_channels, hidden_channels, heads=2, dropout=0.6)\n",
        "            self.conv2 = GATConv(hidden_channels * 2, hidden_channels, heads=2, dropout=0.6)\n",
        "            self.conv3 = GATConv(hidden_channels * 2, out_channels, heads=1, concat=False, dropout=0.6)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported GNN type. Choose from 'gcn', 'sage', 'gat'.\")\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr=None):\n",
        "        if isinstance(self.conv1, GATConv):\n",
        "            x = self.conv1(x, edge_index)\n",
        "        else:\n",
        "            x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "\n",
        "        if isinstance(self.conv2, GATConv):\n",
        "            x = self.conv2(x, edge_index)\n",
        "        else:\n",
        "            x = self.conv2(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "\n",
        "        if isinstance(self.conv3, GATConv):\n",
        "            x = self.conv3(x, edge_index)\n",
        "        else:\n",
        "            x = self.conv3(x, edge_index)\n",
        "        return x  # Output shape: (n_assets, 2)\n",
        "\n",
        "###############################################\n",
        "# 2. Define Functions for Distance Correlation and Adjacency Matrix\n",
        "###############################################\n",
        "\n",
        "def calculate_distance_correlation(price_data: pd.DataFrame) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Calculate the distance correlation matrix for all pairs of assets.\n",
        "    \"\"\"\n",
        "    assets = price_data.columns\n",
        "    n_assets = len(assets)\n",
        "    dist_corr_matrix = np.zeros((n_assets, n_assets), dtype=np.float32)\n",
        "\n",
        "    print(\"Calculating distance correlation matrix...\")\n",
        "    for i in range(n_assets):\n",
        "        for j in range(i + 1, n_assets):\n",
        "            dist_corr = dcor.distance_correlation(price_data.iloc[:, i], price_data.iloc[:, j])\n",
        "            dist_corr_matrix[i, j] = dist_corr\n",
        "            dist_corr_matrix[j, i] = dist_corr  # Symmetric matrix\n",
        "\n",
        "    print(\"Distance correlation matrix calculation complete.\")\n",
        "    return dist_corr_matrix\n",
        "\n",
        "def build_weighted_adjacency_matrix(dist_corr_matrix: np.ndarray, threshold: float) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Build a weighted adjacency matrix based on distance correlation threshold.\n",
        "    \"\"\"\n",
        "    print(f\"Building weighted adjacency matrix with threshold={threshold}...\")\n",
        "    adjacency = np.where(dist_corr_matrix >= threshold, dist_corr_matrix, 0.0)\n",
        "    np.fill_diagonal(adjacency, 0.0)  # Remove self-loops\n",
        "    print(\"Weighted adjacency matrix construction complete.\")\n",
        "    return adjacency\n",
        "\n",
        "def build_unweighted_adjacency_matrix(dist_corr_matrix: np.ndarray, threshold: float) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Build an unweighted adjacency matrix based on distance correlation threshold.\n",
        "    \"\"\"\n",
        "    print(f\"Building unweighted adjacency matrix with threshold={threshold}...\")\n",
        "    adjacency = np.where(dist_corr_matrix >= threshold, 1.0, 0.0)\n",
        "    np.fill_diagonal(adjacency, 0.0)  # Remove self-loops\n",
        "    print(\"Unweighted adjacency matrix construction complete.\")\n",
        "    return adjacency\n",
        "\n",
        "###############################################\n",
        "# 3. Feature Engineering\n",
        "###############################################\n",
        "\n",
        "def engineer_node_features(price_data: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Engineer node features including average return, volatility, momentum, etc.\n",
        "    \"\"\"\n",
        "    print(\"Engineering enhanced node features...\")\n",
        "    # Calculate daily returns\n",
        "    returns = price_data.pct_change().dropna()\n",
        "\n",
        "    # Feature 1: Average Historical Return\n",
        "    avg_return = returns.mean().values.reshape(-1, 1)\n",
        "\n",
        "    # Feature 2: Historical Volatility\n",
        "    volatility = returns.std().values.reshape(-1, 1)\n",
        "\n",
        "    # Feature 3: 5-Day Momentum\n",
        "    momentum_5d = price_data.pct_change(periods=5).mean().values.reshape(-1, 1)\n",
        "\n",
        "    # Feature 4: 10-Day Momentum\n",
        "    momentum_10d = price_data.pct_change(periods=10).mean().values.reshape(-1, 1)\n",
        "\n",
        "    # Feature 5: Maximum Drawdown (past 30 days)\n",
        "    rolling_max = price_data.rolling(window=30).max()\n",
        "    drawdown = (price_data / rolling_max) - 1.0\n",
        "    max_drawdown = drawdown.min().values.reshape(-1, 1)\n",
        "\n",
        "    # Feature 6: Sharpe Ratio (assuming risk-free rate = 0)\n",
        "    sharpe_ratio = (returns.mean().values.reshape(-1,1)) / (returns.std().values.reshape(-1,1) + 1e-15)\n",
        "\n",
        "    # Combine all features\n",
        "    features = np.hstack((avg_return, volatility, momentum_5d, momentum_10d, max_drawdown, sharpe_ratio))\n",
        "    feature_df = pd.DataFrame(features, columns=['avg_return', 'volatility', 'momentum_5d', 'momentum_10d', 'max_drawdown', 'sharpe_ratio'])\n",
        "\n",
        "    # Normalize features\n",
        "    feature_df = (feature_df - feature_df.mean()) / feature_df.std()\n",
        "\n",
        "    print(\"Advanced feature engineering complete.\")\n",
        "    return feature_df\n",
        "\n",
        "###############################################\n",
        "# 4. Prepare Graph Data\n",
        "###############################################\n",
        "\n",
        "def prepare_graph_data(price_data: pd.DataFrame, deltaR: np.ndarray, deltaSigma: np.ndarray, threshold: float, use_weighted_adj: bool = True) -> Data:\n",
        "    \"\"\"\n",
        "    Prepare the PyG Data object for GNN training.\n",
        "    \"\"\"\n",
        "    # Calculate distance correlation matrix\n",
        "    dist_corr_matrix = calculate_distance_correlation(price_data)\n",
        "\n",
        "    # Build adjacency matrix based on threshold\n",
        "    if use_weighted_adj:\n",
        "        adjacency = build_weighted_adjacency_matrix(dist_corr_matrix, threshold)\n",
        "    else:\n",
        "        adjacency = build_unweighted_adjacency_matrix(dist_corr_matrix, threshold)\n",
        "\n",
        "    # Feature Engineering\n",
        "    node_features = engineer_node_features(price_data)\n",
        "\n",
        "    # Prepare targets\n",
        "    Y = np.column_stack((deltaR, deltaSigma))  # shape: (n_assets, 2)\n",
        "\n",
        "    # Create edge_index\n",
        "    edges = np.array(adjacency.nonzero())\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long)\n",
        "\n",
        "    # Create PyG Data object\n",
        "    data = Data(x=torch.tensor(node_features.values, dtype=torch.float),\n",
        "                edge_index=edge_index,\n",
        "                y=torch.tensor(Y, dtype=torch.float))\n",
        "\n",
        "    return data\n",
        "\n",
        "###############################################\n",
        "# 5. Threshold Selection via Cross-Validation\n",
        "###############################################\n",
        "\n",
        "def cross_validate_threshold(price_data: pd.DataFrame, deltaR: np.ndarray, deltaSigma: np.ndarray, thresholds: list, gnn_type: str = 'gcn', hidden_channels: int = 32, epochs: int = 50, lr: float = 1e-3) -> float:\n",
        "    \"\"\"\n",
        "    Perform cross-validation to select the optimal threshold for adjacency matrix.\n",
        "    \"\"\"\n",
        "    print(\"Starting threshold selection via cross-validation...\")\n",
        "    tscv = TimeSeriesSplit(n_splits=3)\n",
        "    n_assets = price_data.shape[1]\n",
        "    asset_indices = np.arange(n_assets)\n",
        "\n",
        "    threshold_rmse = {}\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        print(f\"Evaluating threshold: {threshold}\")\n",
        "        rmses = []\n",
        "\n",
        "        for train_idx, val_idx in tscv.split(asset_indices):\n",
        "            train_assets = asset_indices[train_idx]\n",
        "            val_assets = asset_indices[val_idx]\n",
        "\n",
        "            # Prepare training data\n",
        "            train_price = price_data.iloc[:, train_assets]\n",
        "            train_deltaR = deltaR[train_assets]\n",
        "            train_deltaSigma = deltaSigma[train_assets]\n",
        "\n",
        "            train_data = prepare_graph_data(train_price, train_deltaR, train_deltaSigma, threshold, use_weighted_adj=True)\n",
        "\n",
        "            # Initialize GNN model\n",
        "            model = MarginalGNN(in_channels=train_data.num_features, hidden_channels=hidden_channels, out_channels=2, gnn_type=gnn_type)\n",
        "\n",
        "            # Train the model\n",
        "            model = train_gnn(model, train_data, epochs=epochs, lr=lr)\n",
        "\n",
        "            # Prepare validation data\n",
        "            val_price = price_data.iloc[:, val_assets]\n",
        "            val_deltaR = deltaR[val_assets]\n",
        "            val_deltaSigma = deltaSigma[val_assets]\n",
        "\n",
        "            val_data = prepare_graph_data(val_price, val_deltaR, val_deltaSigma, threshold, use_weighted_adj=True)\n",
        "\n",
        "            # Evaluate the model on validation data\n",
        "            rmse, _ = evaluate_gnn(model, val_data)\n",
        "            rmses.append(rmse)\n",
        "\n",
        "        avg_rmse = np.mean(rmses)\n",
        "        threshold_rmse[threshold] = avg_rmse\n",
        "        print(f\"Threshold {threshold} => Average RMSE: {avg_rmse:.6f}\")\n",
        "\n",
        "    # Select the threshold with the lowest average RMSE\n",
        "    optimal_threshold = min(threshold_rmse, key=threshold_rmse.get)\n",
        "    print(f\"Optimal threshold selected: {optimal_threshold} with RMSE: {threshold_rmse[optimal_threshold]:.6f}\")\n",
        "    return optimal_threshold\n",
        "\n",
        "###############################################\n",
        "# 6. Training and Evaluation Functions\n",
        "###############################################\n",
        "\n",
        "def train_gnn(model: nn.Module, data: Data, epochs: int = 100, lr: float = 1e-3) -> nn.Module:\n",
        "    \"\"\"\n",
        "    Train the GNN model.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): GNN model to train.\n",
        "        data (torch_geometric.data.Data): Graph data.\n",
        "        epochs (int): Number of training epochs.\n",
        "        lr (float): Learning rate.\n",
        "\n",
        "    Returns:\n",
        "        nn.Module: Trained model.\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    data = data.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index)  # shape: (n_assets, 2)\n",
        "        loss = criterion(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 10 == 0 or epoch == 1:\n",
        "            print(f\"Epoch {epoch}/{epochs}, Loss: {loss.item():.6f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def evaluate_gnn(model: nn.Module, data: Data) -> tuple:\n",
        "    \"\"\"\n",
        "    Evaluate the GNN model using RMSE.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Trained GNN model.\n",
        "        data (torch_geometric.data.Data): Graph data.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (RMSE score, Predicted marginal contributions as np.ndarray)\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    data = data.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = model(data.x, data.edge_index)\n",
        "\n",
        "    preds = preds.cpu().numpy()\n",
        "    targets = data.y.cpu().numpy()\n",
        "\n",
        "    rmse = sqrt(mean_squared_error(targets, preds))\n",
        "    return rmse, preds\n",
        "\n",
        "###############################################\n",
        "# 7. Full GNN Workflow Integration Function\n",
        "###############################################\n",
        "\n",
        "def full_gnn_workflow(price_data: pd.DataFrame, deltaR: np.ndarray, deltaSigma: np.ndarray):\n",
        "    \"\"\"\n",
        "    Complete workflow to train and evaluate GNNs with different aggregators.\n",
        "    \"\"\"\n",
        "    # Step 1: Define a range of thresholds to evaluate\n",
        "    thresholds = [0.2, 0.3, 0.4, 0.5]\n",
        "\n",
        "    # Step 2: Perform cross-validation to select the optimal threshold\n",
        "    optimal_threshold = cross_validate_threshold(price_data, deltaR, deltaSigma, thresholds, gnn_type='gcn', hidden_channels=32, epochs=50, lr=1e-3)\n",
        "\n",
        "    # Step 3: Prepare the final graph data using the optimal threshold\n",
        "    data = prepare_graph_data(price_data, deltaR, deltaSigma, optimal_threshold, use_weighted_adj=True)\n",
        "\n",
        "    # Debugging: Check uniqueness of targets\n",
        "    unique_deltaR = np.unique(data.y[:,0])\n",
        "    unique_deltaSigma = np.unique(data.y[:,1])\n",
        "    print(f\"Unique DeltaR values: {unique_deltaR}\")\n",
        "    print(f\"Unique DeltaSigma values: {unique_deltaSigma}\")\n",
        "\n",
        "    if len(unique_deltaR) == 1 or len(unique_deltaSigma) == 1:\n",
        "        print(\"Warning: All target DeltaR or DeltaSigma values are identical. GNN may not learn effectively.\")\n",
        "\n",
        "    # Step 4: Define aggregator types to evaluate\n",
        "    aggregators = ['gcn', 'sage', 'gat']\n",
        "\n",
        "    # Step 5: Initialize a dictionary to store results\n",
        "    results = {}\n",
        "\n",
        "    # Step 6: Train and evaluate GNN models for each aggregator\n",
        "    for agg in aggregators:\n",
        "        print(f\"Training GNN with aggregator: {agg.upper()}\")\n",
        "\n",
        "        # Initialize the model\n",
        "        model = MarginalGNN(in_channels=data.num_features, hidden_channels=32, out_channels=2, gnn_type=agg)\n",
        "\n",
        "        # Train the model\n",
        "        trained_model = train_gnn(model, data, epochs=100, lr=1e-3)\n",
        "\n",
        "        # Evaluate the model\n",
        "        rmse, preds = evaluate_gnn(trained_model, data)\n",
        "\n",
        "        # Store the results\n",
        "        results[agg] = {\n",
        "            'model': trained_model,\n",
        "            'rmse': rmse,\n",
        "            'predictions': preds\n",
        "        }\n",
        "\n",
        "        print(f\"GNN with {agg.upper()} Aggregator => RMSE: {rmse:.6f}\")\n",
        "\n",
        "    # Step 7: Determine the best aggregator based on RMSE\n",
        "    best_aggregator = min(results, key=lambda x: results[x]['rmse'])\n",
        "    best_rmse = results[best_aggregator]['rmse']\n",
        "\n",
        "    print(f\"Best Aggregator: {best_aggregator.upper()} with RMSE: {best_rmse:.6f}\")\n",
        "\n",
        "    # Optional: Display predictions of the best model\n",
        "    best_preds = results[best_aggregator]['predictions']\n",
        "    pred_df = pd.DataFrame(best_preds, columns=['DeltaR_Pred', 'DeltaSigma_Pred'], index=price_data.columns)\n",
        "    print(\"Predicted Marginal Contributions (Best Model):\")\n",
        "    print(pred_df)\n",
        "\n",
        "    # Visualization: Actual vs Predicted\n",
        "    plt.figure(figsize=(12,5))\n",
        "\n",
        "    # DeltaR\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.scatter(data.y[:,0], best_preds[:,0], color='blue')\n",
        "    plt.xlabel('Actual DeltaR')\n",
        "    plt.ylabel('Predicted DeltaR')\n",
        "    plt.title(f'GNN Predictions vs Actual DeltaR ({best_aggregator.upper()})')\n",
        "    plt.plot([data.y[:,0].min(), data.y[:,0].max()], [data.y[:,0].min(), data.y[:,0].max()], 'r--')\n",
        "\n",
        "    # DeltaSigma\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.scatter(data.y[:,1], best_preds[:,1], color='green')\n",
        "    plt.xlabel('Actual DeltaSigma')\n",
        "    plt.ylabel('Predicted DeltaSigma')\n",
        "    plt.title(f'GNN Predictions vs Actual DeltaSigma ({best_aggregator.upper()})')\n",
        "    plt.plot([data.y[:,1].min(), data.y[:,1].max()], [data.y[:,1].min(), data.y[:,1].max()], 'r--')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Visualization: Bar Charts\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.bar(pred_df.index, pred_df['DeltaR_Pred'], color='skyblue')\n",
        "    plt.xlabel('Assets')\n",
        "    plt.ylabel('DeltaR Predicted')\n",
        "    plt.title('GNN Predicted DeltaR per Asset')\n",
        "    plt.xticks(rotation=90)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.bar(pred_df.index, pred_df['DeltaSigma_Pred'], color='salmon')\n",
        "    plt.xlabel('Assets')\n",
        "    plt.ylabel('DeltaSigma Predicted')\n",
        "    plt.title('GNN Predicted DeltaSigma per Asset')\n",
        "    plt.xticks(rotation=90)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return results\n",
        "\n",
        "###############################################\n",
        "# 8. Example Usage with Real Data\n",
        "###############################################\n",
        "\n",
        "# Ensure that 'xgb_models' and 'nn_models' dictionaries are available\n",
        "if 'xgb_models' not in globals() or 'nn_models' not in globals():\n",
        "    print(\"Classic ML models not found. Please run the Step 2 cell first!\")\n",
        "else:\n",
        "    print(\"Generating DeltaR and DeltaSigma using trained Neural Network and XGBoost models...\")\n",
        "    n_assets = price_data.shape[1]\n",
        "\n",
        "    # Initialize arrays to store predictions\n",
        "    deltaR_final = np.zeros(n_assets)\n",
        "    deltaSigma_final = np.zeros(n_assets)\n",
        "\n",
        "    # Initialize lists to store RMSEs for dynamic weighting\n",
        "    rmse_r_nn = []\n",
        "    rmse_r_xgb = []\n",
        "    rmse_s_nn = []\n",
        "    rmse_s_xgb = []\n",
        "\n",
        "    # Calculate RMSEs for weighting based on training data\n",
        "\n",
        "    for asset in price_data.columns:\n",
        "        # Neural Network RMSE for DeltaR\n",
        "        nn_pred_r_train = nn_models['DeltaR'][asset].predict(X_train)\n",
        "        rmse_r_nn.append(sqrt(mean_squared_error(y_train[asset], nn_pred_r_train)))\n",
        "\n",
        "        # Neural Network RMSE for DeltaSigma\n",
        "        nn_pred_s_train = nn_models['DeltaSigma'][asset].predict(X_train)\n",
        "        rmse_s_nn.append(sqrt(mean_squared_error(y_train[asset], nn_pred_s_train)))\n",
        "\n",
        "        # XGBoost RMSE for DeltaR\n",
        "        xgb_pred_r_train = xgb_models['DeltaR'][asset].predict(X_train)\n",
        "        rmse_r_xgb.append(sqrt(mean_squared_error(y_train[asset], xgb_pred_r_train)))\n",
        "\n",
        "        # XGBoost RMSE for DeltaSigma\n",
        "        xgb_pred_s_train = xgb_models['DeltaSigma'][asset].predict(X_train)\n",
        "        rmse_s_xgb.append(sqrt(mean_squared_error(y_train[asset], xgb_pred_s_train)))\n",
        "\n",
        "    # Convert lists to numpy arrays for vectorized operations\n",
        "    rmse_r_nn = np.array(rmse_r_nn)\n",
        "    rmse_r_xgb = np.array(rmse_r_xgb)\n",
        "    rmse_s_nn = np.array(rmse_s_nn)\n",
        "    rmse_s_xgb = np.array(rmse_s_xgb)\n",
        "\n",
        "    # Compute weights inversely proportional to RMSE\n",
        "    weights_r_nn = 1 / (rmse_r_nn + 1e-10)  # Add small value to prevent division by zero\n",
        "    weights_r_xgb = 1 / (rmse_r_xgb + 1e-10)\n",
        "    weights_s_nn = 1 / (rmse_s_nn + 1e-10)\n",
        "    weights_s_xgb = 1 / (rmse_s_xgb + 1e-10)\n",
        "\n",
        "    # Normalize weights so that they sum to 1 for each prediction type\n",
        "    weights_r_total = weights_r_nn + weights_r_xgb\n",
        "    weights_s_total = weights_s_nn + weights_s_xgb\n",
        "\n",
        "    weights_r_nn /= weights_r_total\n",
        "    weights_r_xgb /= weights_r_total\n",
        "    weights_s_nn /= weights_s_total\n",
        "    weights_s_xgb /= weights_s_total\n",
        "\n",
        "    # Log the weights for verification\n",
        "    print(f\"Weights for DeltaR - Neural Networks: {weights_r_nn}\")\n",
        "    print(f\"Weights for DeltaR - XGBoost: {weights_r_xgb}\")\n",
        "    print(f\"Weights for DeltaSigma - Neural Networks: {weights_s_nn}\")\n",
        "    print(f\"Weights for DeltaSigma - XGBoost: {weights_s_xgb}\")\n",
        "\n",
        "    # Collect and combine predictions using dynamic weights\n",
        "    print(\"Collecting and combining predictions for each asset using dynamic weights...\")\n",
        "    for idx, asset in enumerate(price_data.columns):\n",
        "        # Neural Network predictions\n",
        "        nn_pred_r = nn_models['DeltaR'][asset].predict(X_test)[-1]  # DeltaR for the last test sample\n",
        "        nn_pred_s = nn_models['DeltaSigma'][asset].predict(X_test)[-1]  # DeltaSigma for the last test sample\n",
        "\n",
        "        # XGBoost predictions\n",
        "        xgb_pred_r = xgb_models['DeltaR'][asset].predict(X_test)[-1]  # DeltaR\n",
        "        xgb_pred_s = xgb_models['DeltaSigma'][asset].predict(X_test)[-1]  # DeltaSigma\n",
        "\n",
        "        # Combine predictions using dynamic weights\n",
        "        deltaR_final[idx] = (weights_r_nn[idx] * nn_pred_r) + (weights_r_xgb[idx] * xgb_pred_r)\n",
        "        deltaSigma_final[idx] = (weights_s_nn[idx] * nn_pred_s) + (weights_s_xgb[idx] * xgb_pred_s)\n",
        "\n",
        "    # Assign predictions to DataFrame for verification\n",
        "    pred_df = pd.DataFrame({\n",
        "        'DeltaR_Pred': deltaR_final,\n",
        "        'DeltaSigma_Pred': deltaSigma_final\n",
        "    }, index=price_data.columns)\n",
        "\n",
        "    print(\"Combined Predictions after Dynamic Weighted Averaging:\")\n",
        "    print(pred_df)\n",
        "\n",
        "    # Verify that predictions are not identical\n",
        "    if np.all(deltaR_final == deltaR_final[0]) or np.all(deltaSigma_final == deltaSigma_final[0]):\n",
        "        print(\"All predictions are identical. Please check the aggregation method.\")\n",
        "    else:\n",
        "        # Optionally, normalize the targets before passing to GNN\n",
        "        scaler_r = StandardScaler()\n",
        "        scaler_s = StandardScaler()\n",
        "\n",
        "        deltaR_final_scaled = scaler_r.fit_transform(deltaR_final.reshape(-1,1)).flatten()\n",
        "        deltaSigma_final_scaled = scaler_s.fit_transform(deltaSigma_final.reshape(-1,1)).flatten()\n",
        "\n",
        "        # Check uniqueness after scaling\n",
        "        unique_deltaR_scaled = np.unique(deltaR_final_scaled)\n",
        "        unique_deltaSigma_scaled = np.unique(deltaSigma_final_scaled)\n",
        "        print(f\"Unique values in scaled DeltaR: {unique_deltaR_scaled}\")\n",
        "        print(f\"Unique values in scaled DeltaSigma: {unique_deltaSigma_scaled}\")\n",
        "\n",
        "        # Prepare GNN workflow with scaled targets\n",
        "        print(\"Starting GNN workflow with dynamically weighted and scaled DeltaR and DeltaSigma...\")\n",
        "        gnn_results = full_gnn_workflow(price_data, deltaR_final_scaled, deltaSigma_final_scaled)\n",
        "        print(\"GNN integration with Dynamic Weighted Ensemble completed successfully!\")\n"
      ],
      "metadata": {
        "id": "8AOlE6eoioCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "#  CELL 4\n",
        "#  Step 4: Implementing Advanced Optimization Methods with Constraints (Updated)\n",
        "# =======================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import logging\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "# Optimization Libraries\n",
        "try:\n",
        "    import pyswarms as ps\n",
        "except ImportError:\n",
        "    print(\"Installing pyswarms for Particle Swarm Optimization...\")\n",
        "    !pip install pyswarms\n",
        "    import pyswarms as ps\n",
        "\n",
        "# Define Upper and Lower Bounds for each asset\n",
        "# Example: No asset can have less than 2% or more than 20% of the portfolio\n",
        "# Adjust these values as per your requirements\n",
        "lower_bounds = np.array([0.02] * price_data.shape[1])  # 2% lower bound\n",
        "upper_bounds = np.array([0.20] * price_data.shape[1])  # 20% upper bound\n",
        "\n",
        "# Define the objective function with penalty for constraint violations\n",
        "def portfolio_performance_penalized(weights, returns, cov_matrix, risk_aversion=1.0, penalty_factor=1e6):\n",
        "    \"\"\"\n",
        "    Calculate the portfolio performance with penalties for constraint violations.\n",
        "\n",
        "    Args:\n",
        "        weights (np.ndarray): Portfolio weights.\n",
        "        returns (np.ndarray): Expected returns for each asset.\n",
        "        cov_matrix (np.ndarray): Covariance matrix of asset returns.\n",
        "        risk_aversion (float): Risk aversion coefficient.\n",
        "        penalty_factor (float): Penalty factor for constraint violations.\n",
        "\n",
        "    Returns:\n",
        "        float: Penalized objective function value.\n",
        "    \"\"\"\n",
        "    portfolio_return = np.dot(weights, returns)\n",
        "    portfolio_risk = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
        "    objective = -(portfolio_return - risk_aversion * portfolio_risk)\n",
        "\n",
        "    # Penalty for weights not summing to 1\n",
        "    penalty = penalty_factor * (np.sum(weights) - 1) ** 2\n",
        "\n",
        "    # Penalty for weights below lower bounds or above upper bounds\n",
        "    penalty += penalty_factor * np.sum((weights < lower_bounds) * (lower_bounds - weights))\n",
        "    penalty += penalty_factor * np.sum((weights > upper_bounds) * (weights - upper_bounds))\n",
        "\n",
        "    return objective + penalty\n",
        "\n",
        "# Define the constraint: Sum of weights equals 1\n",
        "def weight_constraint(weights):\n",
        "    return np.sum(weights) - 1\n",
        "\n",
        "# Bounds for optimization (0 to 1 for each asset)\n",
        "bounds_qp_sa = [(lb, ub) for lb, ub in zip(lower_bounds, upper_bounds)]\n",
        "\n",
        "# Initial guess: Equal weights within bounds\n",
        "initial_guess = np.array([max(1.0 / price_data.shape[1], lb) for lb in lower_bounds])\n",
        "initial_guess /= np.sum(initial_guess)  # Normalize to sum to 1\n",
        "\n",
        "# Extract returns and covariance from predictions\n",
        "# Extract returns and covariance from predictions\n",
        "expected_returns = deltaR_final_scaled  # Use scaled deltaR\n",
        "covariance_matrix = np.diag(deltaSigma_final_scaled ** 2)  # Use scaled deltaSigma\n",
        "\n",
        "# Risk aversion coefficient\n",
        "lambda_risk = 1.0\n",
        "\n",
        "# 1. Quadratic Programming (Mean-Variance Optimization) using SciPy's minimize\n",
        "def optimize_portfolio_qp(expected_returns, covariance_matrix, risk_aversion=1.0):\n",
        "    constraints = [{'type': 'eq', 'fun': weight_constraint}]\n",
        "    result = minimize(portfolio_performance_penalized, initial_guess,\n",
        "                      args=(expected_returns, covariance_matrix, risk_aversion),\n",
        "                      method='SLSQP',\n",
        "                      bounds=bounds_qp_sa,\n",
        "                      constraints=constraints,\n",
        "                      options={'maxiter': 1000, 'ftol': 1e-9})\n",
        "    if result.success:\n",
        "        optimized_weights = result.x\n",
        "        # Enforce bounds manually (optional redundancy)\n",
        "        optimized_weights = np.clip(optimized_weights, lower_bounds, upper_bounds)\n",
        "        # Normalize weights to sum to 1\n",
        "        optimized_weights /= np.sum(optimized_weights)\n",
        "        portfolio_ret = np.dot(optimized_weights, expected_returns)\n",
        "        portfolio_risk = np.sqrt(np.dot(optimized_weights.T, np.dot(covariance_matrix, optimized_weights)))\n",
        "        print(f\"QP Optimization Success: Return={portfolio_ret:.4f}, Risk={portfolio_risk:.4f}\")\n",
        "        return optimized_weights\n",
        "    else:\n",
        "        print(\"QP Optimization failed.\")\n",
        "        return None\n",
        "\n",
        "# 2. Simulated Annealing (Using SciPy's minimize with SLSQP and penalties)\n",
        "def optimize_portfolio_sa(expected_returns, covariance_matrix, risk_aversion=1.0):\n",
        "    def objective(weights):\n",
        "        return portfolio_performance_penalized(weights, expected_returns, covariance_matrix, risk_aversion)\n",
        "\n",
        "    constraints_sa = [{'type': 'eq', 'fun': weight_constraint}]\n",
        "    result = minimize(objective, initial_guess, method='SLSQP',\n",
        "                      bounds=bounds_qp_sa,\n",
        "                      constraints=constraints_sa,\n",
        "                      options={'maxiter': 1000, 'ftol': 1e-9})\n",
        "    if result.success:\n",
        "        optimized_weights = result.x\n",
        "        # Enforce bounds manually (optional redundancy)\n",
        "        optimized_weights = np.clip(optimized_weights, lower_bounds, upper_bounds)\n",
        "        # Normalize weights to sum to 1\n",
        "        optimized_weights /= np.sum(optimized_weights)\n",
        "        portfolio_ret = np.dot(optimized_weights, expected_returns)\n",
        "        portfolio_risk = np.sqrt(np.dot(optimized_weights.T, np.dot(covariance_matrix, optimized_weights)))\n",
        "        print(f\"Simulated Annealing Success: Return={portfolio_ret:.4f}, Risk={portfolio_risk:.4f}\")\n",
        "        return optimized_weights\n",
        "    else:\n",
        "        print(\"Simulated Annealing Optimization failed.\")\n",
        "        return None\n",
        "\n",
        "# 3. Particle Swarm Optimization using pyswarms with Penalty\n",
        "def optimize_portfolio_pso(expected_returns, covariance_matrix, risk_aversion=1.0):\n",
        "    # Define objective function for PSO (to minimize)\n",
        "    def f(x):\n",
        "        # x is a swarm of particles, each row is a position\n",
        "        obj = np.array([portfolio_performance_penalized(w, expected_returns, covariance_matrix, risk_aversion) for w in x])\n",
        "        return obj\n",
        "\n",
        "    # PSO does not support constraints directly; use penalty method\n",
        "    options = {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
        "    optimizer = ps.single.GlobalBestPSO(n_particles=100, dimensions=price_data.shape[1],\n",
        "                                        options=options, bounds=(lower_bounds, upper_bounds))\n",
        "\n",
        "    best_cost, best_pos = optimizer.optimize(f, iters=200, verbose=False)\n",
        "\n",
        "    # Enforce bounds manually\n",
        "    best_pos = np.clip(best_pos, lower_bounds, upper_bounds)\n",
        "    # Normalize weights to sum to 1\n",
        "    best_pos /= np.sum(best_pos)\n",
        "\n",
        "    portfolio_ret = np.dot(best_pos, expected_returns)\n",
        "    portfolio_risk = np.sqrt(np.dot(best_pos.T, np.dot(covariance_matrix, best_pos)))\n",
        "    print(f\"PSO Optimization Success: Return={portfolio_ret:.4f}, Risk={portfolio_risk:.4f}\")\n",
        "\n",
        "    return best_pos\n",
        "\n",
        "# Execute all optimization methods and store results\n",
        "def execute_all_optimizations(expected_returns, covariance_matrix, risk_aversion=1.0):\n",
        "    optim_results = {}\n",
        "\n",
        "    # 1. Quadratic Programming\n",
        "    optim_results['QP'] = optimize_portfolio_qp(expected_returns, covariance_matrix, risk_aversion)\n",
        "\n",
        "    # 2. Simulated Annealing\n",
        "    optim_results['SA'] = optimize_portfolio_sa(expected_returns, covariance_matrix, risk_aversion)\n",
        "\n",
        "    # 3. Particle Swarm Optimization\n",
        "    optim_results['PSO'] = optimize_portfolio_pso(expected_returns, covariance_matrix, risk_aversion)\n",
        "\n",
        "    # You can add more optimization methods here following the same pattern\n",
        "\n",
        "    return optim_results\n",
        "\n",
        "# Example Usage\n",
        "if 'expected_returns' not in globals() or 'covariance_matrix' not in globals():\n",
        "    print(\"expected_returns or covariance_matrix not defined. Please ensure Step 2 and 3 are executed correctly.\")\n",
        "else:\n",
        "    print(\"Executing all optimization methods with constraints...\")\n",
        "    optim_results = execute_all_optimizations(expected_returns, covariance_matrix, risk_aversion=lambda_risk)\n",
        "\n",
        "    # Display the optimized weights and their performance\n",
        "    for method, weights in optim_results.items():\n",
        "        if weights is not None:\n",
        "            portfolio_ret = np.dot(weights, expected_returns)\n",
        "            portfolio_risk = np.sqrt(np.dot(weights.T, np.dot(covariance_matrix, weights)))\n",
        "            print(f\"{method} Optimization => Return: {portfolio_ret:.4f}, Risk: {portfolio_risk:.4f}\")\n",
        "            print(f\"\\n{method} Optimized Weights:\")\n",
        "            weights_df = pd.DataFrame(weights, index=price_data.columns, columns=['Weight'])\n",
        "            print(weights_df)\n",
        "        else:\n",
        "            print(f\"{method} Optimization did not return valid weights.\")\n",
        "\n",
        "    print(\"All optimization methods executed successfully with constraints!\")\n"
      ],
      "metadata": {
        "id": "J0W7XoCWIQho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "#  CELL 5 - Updated\n",
        "#  Step 5: Iterative Partial-Derivative Layering Approach (Updated)\n",
        "# =======================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import logging\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "# Define convergence criteria\n",
        "def check_convergence(old_ret, new_ret, threshold=1e-4):\n",
        "    return np.abs(new_ret - old_ret) < threshold\n",
        "\n",
        "# Iterative Optimization Function\n",
        "def iterative_portfolio_optimization(price_data, initial_weights, deltaR, deltaSigma, covariance_matrix, risk_aversion=1.0, max_iterations=10, threshold=1e-4):\n",
        "    \"\"\"\n",
        "    Iteratively optimize the portfolio by recalculating marginal contributions.\n",
        "\n",
        "    Args:\n",
        "        price_data (pd.DataFrame): Asset price data.\n",
        "        initial_weights (np.ndarray): Initial portfolio weights.\n",
        "        deltaR (np.ndarray): Marginal return contributions from ML models.\n",
        "        deltaSigma (np.ndarray): Marginal risk contributions from ML models.\n",
        "        covariance_matrix (np.ndarray): Covariance matrix.\n",
        "        risk_aversion (float): Risk aversion coefficient.\n",
        "        max_iterations (int): Maximum number of iterations.\n",
        "        threshold (float): Convergence threshold.\n",
        "\n",
        "    Returns:\n",
        "        dict: Final optimized weights and performance metrics.\n",
        "    \"\"\"\n",
        "    weights = initial_weights.copy()\n",
        "    portfolio_ret, portfolio_risk = np.dot(weights, deltaR), np.sqrt(np.dot(weights.T, np.dot(covariance_matrix, weights)))\n",
        "    print(f\"Initial Portfolio => Return: {portfolio_ret:.4f}, Risk: {portfolio_risk:.4f}\")\n",
        "\n",
        "    for iteration in range(1, max_iterations + 1):\n",
        "        print(f\"\\n--- Iteration {iteration} ---\")\n",
        "\n",
        "        # Step 2: Optimize portfolio with updated deltaR and deltaSigma\n",
        "        optim_results = execute_all_optimizations(deltaR, covariance_matrix, risk_aversion)\n",
        "\n",
        "        # Select the method with the lowest RMSE between predicted and actual deltaR\n",
        "        # Calculate RMSE for each method\n",
        "        rmse_scores = {}\n",
        "        for method, weights_opt in optim_results.items():\n",
        "            if weights_opt is not None:\n",
        "                predicted_deltaR = weights_opt * deltaR\n",
        "                rmse = sqrt(mean_squared_error(deltaR, predicted_deltaR))\n",
        "                rmse_scores[method] = rmse\n",
        "            else:\n",
        "                rmse_scores[method] = np.inf  # Assign infinity if optimization failed\n",
        "\n",
        "        # Select the method with the lowest RMSE\n",
        "        selected_method = min(rmse_scores, key=rmse_scores.get)\n",
        "        new_weights = optim_results[selected_method]\n",
        "\n",
        "        # Calculate new portfolio performance\n",
        "        new_portfolio_ret = np.dot(new_weights, deltaR)\n",
        "        new_portfolio_risk = np.sqrt(np.dot(new_weights.T, np.dot(covariance_matrix, new_weights)))\n",
        "        print(f\"{selected_method} Optimization => Return: {new_portfolio_ret:.4f}, Risk: {new_portfolio_risk:.4f}\")\n",
        "        print(f\"Selected Method RMSE: {rmse_scores[selected_method]:.6f}\")\n",
        "\n",
        "        # Check for convergence\n",
        "        if check_convergence(portfolio_ret, new_portfolio_ret, threshold):\n",
        "            print(\"Convergence achieved.\")\n",
        "            weights = new_weights\n",
        "            portfolio_ret, portfolio_risk = new_portfolio_ret, new_portfolio_risk\n",
        "            break\n",
        "        else:\n",
        "            weights = new_weights\n",
        "            portfolio_ret, portfolio_risk = new_portfolio_ret, new_portfolio_risk\n",
        "\n",
        "    print(\"\\n--- Iterative Optimization Completed ---\")\n",
        "    print(f\"Final Portfolio => Return: {portfolio_ret:.4f}, Risk: {portfolio_risk:.4f}\")\n",
        "    print(\"Final Optimized Weights:\")\n",
        "    weights_df = pd.DataFrame(weights, index=price_data.columns, columns=['Weight'])\n",
        "    print(weights_df)\n",
        "\n",
        "    return {\n",
        "        'weights': weights,\n",
        "        'return': portfolio_ret,\n",
        "        'risk': portfolio_risk\n",
        "    }\n",
        "\n",
        "# Example Usage\n",
        "if 'optim_results' not in globals() or not optim_results:\n",
        "    print(\"Optimization results not found. Please ensure Step 4 is executed correctly.\")\n",
        "else:\n",
        "    # Select the best method based on RMSE\n",
        "    # Calculate RMSE for each method\n",
        "    rmse_scores = {}\n",
        "    for method, weights_opt in optim_results.items():\n",
        "        if weights_opt is not None:\n",
        "            predicted_deltaR = weights_opt * deltaR_final_scaled\n",
        "            rmse = sqrt(mean_squared_error(deltaR_final_scaled, predicted_deltaR))\n",
        "            rmse_scores[method] = rmse\n",
        "        else:\n",
        "            rmse_scores[method] = np.inf  # Assign infinity if optimization failed\n",
        "\n",
        "    # Select method with lowest RMSE\n",
        "    best_method = min(rmse_scores, key=rmse_scores.get)\n",
        "    best_weights = optim_results[best_method]\n",
        "    print(f\"Selected Best Method for Iterative Optimization: {best_method} with RMSE={rmse_scores[best_method]:.6f}\")\n",
        "\n",
        "    # Execute iterative optimization\n",
        "    final_portfolio = iterative_portfolio_optimization(\n",
        "        price_data,\n",
        "        best_weights,\n",
        "        deltaR_final_scaled,\n",
        "        deltaSigma_final_scaled,\n",
        "        covariance_matrix,\n",
        "        risk_aversion=lambda_risk,\n",
        "        max_iterations=10,\n",
        "        threshold=1e-4\n",
        "    )\n",
        "\n",
        "    print(\"Iterative Partial-Derivative Layering Approach completed successfully!\")\n"
      ],
      "metadata": {
        "id": "-SKAsdJhIcnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "#  CELL 6\n",
        "#  Step 6: Finalizing ACHL Integration with Advanced Performance Metrics\n",
        "# =======================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import logging\n",
        "from scipy.stats import linregress\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "# Configure logging if not already done\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define Performance Metrics Functions\n",
        "\n",
        "def sortino_ratio(returns, target=0.0):\n",
        "    \"\"\"\n",
        "    Calculate the Sortino Ratio.\n",
        "\n",
        "    Args:\n",
        "        returns (np.ndarray or pd.Series): Portfolio returns.\n",
        "        target (float): Target or minimum acceptable return.\n",
        "\n",
        "    Returns:\n",
        "        float: Sortino Ratio.\n",
        "    \"\"\"\n",
        "    excess_returns = returns - target\n",
        "    downside_returns = excess_returns[excess_returns < 0]\n",
        "    downside_std = np.sqrt(np.mean(downside_returns ** 2))\n",
        "    if downside_std == 0:\n",
        "        return np.nan\n",
        "    return np.mean(excess_returns) / downside_std\n",
        "\n",
        "def calculate_alpha_beta(portfolio_returns, benchmark_returns):\n",
        "    \"\"\"\n",
        "    Calculate Alpha and Beta of the portfolio against a benchmark.\n",
        "\n",
        "    Args:\n",
        "        portfolio_returns (pd.Series): Portfolio returns.\n",
        "        benchmark_returns (pd.Series): Benchmark returns.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (Alpha, Beta)\n",
        "    \"\"\"\n",
        "    slope, intercept, r_value, p_value, std_err = linregress(benchmark_returns, portfolio_returns)\n",
        "    beta = slope\n",
        "    alpha = intercept\n",
        "    return alpha, beta\n",
        "\n",
        "def information_ratio(portfolio_returns, benchmark_returns):\n",
        "    \"\"\"\n",
        "    Calculate the Information Ratio of the portfolio.\n",
        "\n",
        "    Args:\n",
        "        portfolio_returns (pd.Series): Portfolio returns.\n",
        "        benchmark_returns (pd.Series): Benchmark returns.\n",
        "\n",
        "    Returns:\n",
        "        float: Information Ratio.\n",
        "    \"\"\"\n",
        "    active_returns = portfolio_returns - benchmark_returns\n",
        "    tracking_error = np.std(active_returns)\n",
        "    if tracking_error == 0:\n",
        "        return np.nan\n",
        "    return np.mean(active_returns) / tracking_error\n",
        "\n",
        "# Benchmark Portfolios\n",
        "\n",
        "def get_benchmark_returns(price_data, expected_returns, covariance_matrix):\n",
        "    \"\"\"\n",
        "    Calculate benchmark returns using various portfolio strategies.\n",
        "\n",
        "    Args:\n",
        "        price_data (pd.DataFrame): Asset price data.\n",
        "        expected_returns (np.ndarray): Expected returns vector.\n",
        "        covariance_matrix (np.ndarray): Covariance matrix.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (dict of benchmark returns, dict of benchmark weights)\n",
        "    \"\"\"\n",
        "    # 1. Equal-Weighted Portfolio\n",
        "    equal_weights = np.array([1.0 / price_data.shape[1]] * price_data.shape[1])\n",
        "    equal_returns = calculate_portfolio_returns(equal_weights, price_data)\n",
        "\n",
        "    # 2. Minimum Variance Portfolio (Efficient Frontier)\n",
        "    def minimum_variance_portfolio(expected_returns, cov_matrix):\n",
        "        num_assets = len(expected_returns)\n",
        "        args = (cov_matrix,)\n",
        "        constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
        "        bounds_mv = tuple((0, 1) for asset in range(num_assets))\n",
        "        initial_guess_mv = np.array([1.0 / num_assets] * num_assets)\n",
        "        result = minimize(portfolio_variance, initial_guess_mv, args=args,\n",
        "                          method='SLSQP', bounds=bounds_mv, constraints=constraints)\n",
        "        if result.success:\n",
        "            return result.x\n",
        "        else:\n",
        "            print(\"Minimum Variance Portfolio Optimization failed.\")\n",
        "            return equal_weights  # Fallback to Equal-Weighted\n",
        "\n",
        "    def portfolio_variance(weights, cov_matrix):\n",
        "        return np.dot(weights.T, np.dot(cov_matrix, weights))\n",
        "\n",
        "    min_var_weights = minimum_variance_portfolio(expected_returns, covariance_matrix)\n",
        "    min_var_returns = calculate_portfolio_returns(min_var_weights, price_data)\n",
        "\n",
        "\n",
        "    # 3. Sparse Portfolio using Lasso Regression\n",
        "    # Reshape expected_returns to match the number of samples in price_data\n",
        "    expected_returns_reshaped = np.tile(expected_returns, (price_data.shape[0], 1))\n",
        "\n",
        "    lasso = Lasso(alpha=0.1)\n",
        "    lasso.fit(price_data, expected_returns_reshaped)  # Use reshaped expected returns\n",
        "\n",
        "    # Modification to ensure correct shape for sparse_weights\n",
        "    sparse_weights = np.abs(lasso.coef_)\n",
        "    sparse_weights = sparse_weights.mean(axis=0)  # Averaging weights across time\n",
        "    sparse_weights = sparse_weights / np.sum(sparse_weights) if np.sum(sparse_weights) != 0 else np.array([1.0 / price_data.shape[1]] * price_data.shape[1])\n",
        "\n",
        "    # Reshape to (20,) if not already in that shape\n",
        "    if sparse_weights.shape != (price_data.shape[1],):\n",
        "      sparse_weights = sparse_weights.reshape(price_data.shape[1],)\n",
        "\n",
        "    sparse_returns = calculate_portfolio_returns(sparse_weights, price_data)\n",
        "\n",
        "\n",
        "    benchmark_weights = {\n",
        "        'Equal-Weighted': equal_weights,\n",
        "        'Minimum-Variance': min_var_weights,\n",
        "        'Sparse': sparse_weights\n",
        "    }\n",
        "\n",
        "    benchmark_returns = {\n",
        "        'Equal-Weighted': equal_returns,\n",
        "        'Minimum-Variance': min_var_returns,\n",
        "        'Sparse': sparse_returns\n",
        "    }\n",
        "\n",
        "    return benchmark_returns, benchmark_weights\n",
        "\n",
        "# Calculate Portfolio Returns\n",
        "\n",
        "def calculate_portfolio_returns(weights, price_data):\n",
        "    \"\"\"\n",
        "    Calculate the portfolio returns given weights and price data.\n",
        "\n",
        "    Args:\n",
        "        weights (np.ndarray): Portfolio weights.\n",
        "        price_data (pd.DataFrame): Asset price data.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: Portfolio returns.\n",
        "    \"\"\"\n",
        "    portfolio_prices = price_data.dot(weights)\n",
        "    portfolio_returns = portfolio_prices.pct_change().dropna()\n",
        "    return portfolio_returns\n",
        "\n",
        "# Calculate Performance Metrics\n",
        "\n",
        "def calculate_performance_metrics(portfolio_returns, benchmark_returns):\n",
        "    \"\"\"\n",
        "    Calculate performance metrics for the portfolio.\n",
        "\n",
        "    Args:\n",
        "        portfolio_returns (pd.Series): Portfolio returns.\n",
        "        benchmark_returns (pd.Series): Benchmark returns.\n",
        "\n",
        "    Returns:\n",
        "        dict: Performance metrics.\n",
        "    \"\"\"\n",
        "    sortino = sortino_ratio(portfolio_returns)\n",
        "    alpha, beta = calculate_alpha_beta(portfolio_returns, benchmark_returns)\n",
        "    info_ratio = information_ratio(portfolio_returns, benchmark_returns)\n",
        "\n",
        "    metrics = {\n",
        "        'Sortino Ratio': sortino,\n",
        "        'Alpha': alpha,\n",
        "        'Beta': beta,\n",
        "        'Information Ratio': info_ratio\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "# Finalize Performance Metrics for All Portfolios\n",
        "\n",
        "def finalize_portfolio_performance(achl_weights, price_data, expected_returns, covariance_matrix, risk_aversion=1.0):\n",
        "    \"\"\"\n",
        "    Calculate and log advanced performance metrics for the ACHL and benchmark portfolios.\n",
        "\n",
        "    Args:\n",
        "        achl_weights (np.ndarray): Optimized ACHL portfolio weights.\n",
        "        price_data (pd.DataFrame): Asset price data.\n",
        "        expected_returns (np.ndarray): Expected returns vector.\n",
        "        covariance_matrix (np.ndarray): Covariance matrix.\n",
        "        risk_aversion (float): Risk aversion coefficient.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Performance metrics for all portfolios.\n",
        "    \"\"\"\n",
        "    performance_metrics = {}\n",
        "\n",
        "    # 1. ACHL Portfolio\n",
        "    achl_returns = calculate_portfolio_returns(achl_weights, price_data)\n",
        "    performance_metrics['ACHL'] = {}\n",
        "    performance_metrics['ACHL']['Return'] = np.mean(achl_returns)\n",
        "    performance_metrics['ACHL']['Risk'] = np.std(achl_returns)\n",
        "    performance_metrics['ACHL']['Sortino Ratio'] = sortino_ratio(achl_returns)\n",
        "\n",
        "    # 2. Benchmark Portfolios\n",
        "    benchmark_returns_dict, benchmark_weights_dict = get_benchmark_returns(price_data, expected_returns, covariance_matrix)\n",
        "\n",
        "    # For Alpha and Beta, define a common market benchmark. Here, we'll use the Minimum-Variance Portfolio as a proxy.\n",
        "    # Replace this with an actual market index for more accurate results.\n",
        "    market_returns = benchmark_returns_dict['Minimum-Variance']\n",
        "\n",
        "    for name, returns in benchmark_returns_dict.items():\n",
        "        sortino = sortino_ratio(returns)\n",
        "        alpha, beta = calculate_alpha_beta(returns, market_returns)\n",
        "        info_ratio = information_ratio(returns, market_returns)\n",
        "\n",
        "        performance_metrics[name] = {\n",
        "            'Return': np.mean(returns),\n",
        "            'Risk': np.std(returns),\n",
        "            'Sortino Ratio': sortino,\n",
        "            'Alpha': alpha,\n",
        "            'Beta': beta,\n",
        "            'Information Ratio': info_ratio\n",
        "        }\n",
        "\n",
        "    # 3. ACHL Performance Metrics\n",
        "    achl_alpha, achl_beta = calculate_alpha_beta(achl_returns, market_returns)\n",
        "    achl_info_ratio = information_ratio(achl_returns, market_returns)\n",
        "\n",
        "    performance_metrics['ACHL']['Alpha'] = achl_alpha\n",
        "    performance_metrics['ACHL']['Beta'] = achl_beta\n",
        "    performance_metrics['ACHL']['Information Ratio'] = achl_info_ratio\n",
        "\n",
        "    # Create a DataFrame for easy comparison\n",
        "    metrics_df = pd.DataFrame(performance_metrics).T\n",
        "    print(\"\\n=== Summary of Performance Metrics ===\")\n",
        "    print(metrics_df)\n",
        "\n",
        "    return metrics_df\n",
        "\n",
        "# Example Usage\n",
        "\n",
        "if 'final_portfolio' not in globals():\n",
        "    print(\"Final portfolio not found. Please ensure Step 5 is executed correctly.\")\n",
        "else:\n",
        "    # Calculate ACHL Portfolio Returns\n",
        "    achl_weights = final_portfolio['weights']\n",
        "    achl_returns = calculate_portfolio_returns(achl_weights, price_data)\n",
        "\n",
        "    # Get Benchmark Returns\n",
        "    benchmark_returns_dict, benchmark_weights_dict = get_benchmark_returns(price_data, expected_returns, covariance_matrix)\n",
        "\n",
        "    # Calculate and log performance metrics\n",
        "    print(\"Finalizing performance metrics for ACHL and Benchmark Portfolios...\")\n",
        "    performance_df = finalize_portfolio_performance(\n",
        "        achl_weights,\n",
        "        price_data,\n",
        "        expected_returns,\n",
        "        covariance_matrix,\n",
        "        risk_aversion=lambda_risk\n",
        "    )\n",
        "\n",
        "    print(\"Advanced performance metrics calculation completed successfully!\")\n"
      ],
      "metadata": {
        "id": "FtNNeuorOJoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "#  CELL 7\n",
        "#  Step 7: Visualization of Portfolio Performance\n",
        "# =======================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_cumulative_returns(price_data, weights_dict, title='Cumulative Returns Comparison'):\n",
        "    \"\"\"\n",
        "    Plot cumulative returns of different portfolios.\n",
        "\n",
        "    Args:\n",
        "        price_data (pd.DataFrame): Asset price data.\n",
        "        weights_dict (dict): Dictionary with portfolio names as keys and weights as values.\n",
        "        title (str): Title of the plot.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    for name, weights in weights_dict.items():\n",
        "        returns = calculate_portfolio_returns(weights, price_data)\n",
        "        cumulative_returns = (1 + returns).cumprod()\n",
        "        plt.plot(cumulative_returns, label=name)\n",
        "\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Cumulative Returns')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Prepare weights dictionary including ACHL and benchmarks\n",
        "weights_dict = {\n",
        "    'ACHL': achl_weights,\n",
        "    'Equal-Weighted': benchmark_weights_dict['Equal-Weighted'],\n",
        "    'Minimum-Variance': benchmark_weights_dict['Minimum-Variance'],\n",
        "    'Sparse': benchmark_weights_dict['Sparse']\n",
        "}\n",
        "\n",
        "# Plot cumulative returns\n",
        "plot_cumulative_returns(price_data, weights_dict, title='Cumulative Returns Comparison: ACHL vs Benchmarks')\n"
      ],
      "metadata": {
        "id": "02nK-YutIdWZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}